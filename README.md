This is pretty much a medium sized NMT and to be honest i dont understand why most of it works
Ive made this with basic concepts of C++ Machine Learning and some help with Chat.gpt(though only in the files and IO aspects since i did this in codespaces)
as for optiazation i used adam since it was quite popular, which is funny since i dont really know the concept
im using lstm and much to my grief im using the codespaces' server cpu to train this frankenstien model, its slow even woth only 100 epoch and 64 batches, i think ill die before it ends training
as for interface im using terminal cuz im lazy and im planning to replace that with voice to text input and well just text ouput since im gonna be creating or borrowing another program for this interface
encoder decoder layers are still shit and this thing runs locally so thats probably one of its saving grace
so yeah this thing is still its brain, maybe next time ill be done with its metophorical appendagges hehe
should've named this bot Frank get it? for Frankenstien.. 
not funny? 
fine
Update; tried running it on my laptop, well it works; had an 90 acc and .2 loss, its my smartest one yet
Test gave good results and translation tolerance fault is well within good ranges, its in a threshold but thats probably due to overfiting since im training this model with just 10,000 sentence pairs, next time though is im gonna train this model for well 100,000+ sentence pairs for 30k epoch
that should do it right?
have to add more too like adding 
